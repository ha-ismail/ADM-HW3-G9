{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import csv\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#from googletrans import Translator\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "#translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading tsvs and getting cleaned stemmed dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the given dataset into pandas for processing using only the needed columns\n",
    "data = pd.read_csv('Airbnb_Texas_Rentals.csv', nrows=10, usecols= ['average_rate_per_night', 'bedrooms_count','city', 'date_of_listing', 'description', 'latitude','longitude','title', 'url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a file for every row in data(the given data of airbnb)\n",
    "fileCount = 0\n",
    "for index, r in data.iterrows():\n",
    "    data_temp = data.loc[index:index]\n",
    "    fileCount +=1\n",
    "    data_temp.to_csv('data/doc_'+str(index+1)+'.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing the functions\n",
    "\n",
    "\n",
    "# list of unique words (terms) to be used to build the vocabulary\n",
    "words = []\n",
    "\n",
    "def cleanData(rawData):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # TODO translatoin\n",
    "    #lang = detect(t4[0])\n",
    "    #print(lang)\n",
    "    #rawData = translator.translate(rawData, dest='en')\n",
    "    \n",
    "    # get words lowercased\n",
    "    t0 = rawData.lower()\n",
    "    # remove puctuations\n",
    "    t1 = tokenizer.tokenize(t0)\n",
    "    #t2Data = stemmer.stem(i for i in t1Data)\n",
    "    #print(len(t1))\n",
    "    # reomve stop words\n",
    "    t2 =[]\n",
    "    t2 = [t1[i] for i in range(0,len(t1)) if t1[i] not in stop_words]\n",
    "    \n",
    "    # stemm words\n",
    "    t3 = [stemmer.stem(t2[i]) for i in range(0, len(t2))]\n",
    "    \n",
    "    # remove nummbers and strings starting with numbers\n",
    "    t4 = [t3[i] for i in range(0, len(t3)) if t3[i][0].isdigit()==False]\n",
    "    \n",
    "\n",
    "    #print(t4)\n",
    "    return(t4)\n",
    "\n",
    "def getWordsTSV(fid):\n",
    "    fileWords=[]\n",
    "    for i in open('data/doc_'+str(fid)+'.tsv'):\n",
    "        #words.append(cleanData(i[0][4]))\n",
    "        data = [i.strip('\\n').split('\\t') for i in open('data/doc_'+str(fid)+'.tsv')]\n",
    "        for w in (cleanData(data[0][4].replace('\\\\n', ' '))):\n",
    "            if w not in fileWords: fileWords.append(w)\n",
    "    return(fileWords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welcom stay privat room queen bed detach bathroom second floor anoth bedroom sofa avail addit guest iah airport pick drop trip\n"
     ]
    }
   ],
   "source": [
    "#print(*getWordsTSV(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcom', 'stay', 'privat', 'room', 'queen', 'bed', 'detach', 'bathroom', 'second', 'floor', 'anoth', 'bedroom', 'sofa', 'avail', 'addit', 'guest', 'iah', 'airport', 'pick', 'drop', 'trip', 'stylish', 'fulli', 'remodel', 'home', 'upscal', 'nw', 'alamo', 'height', 'area', 'amaz', 'locat', 'hous', 'conveni', 'quiet', 'street', 'beauti', 'season', 'tree', 'prestigi', 'neighborhood', 'close', 'loop', 'town', 'featur', 'open', 'plan', 'origin', 'hardwood', 'full', 'independ', 'garden', 'tv', 'sleep', 'european', 'inspir', 'kitchen', 'top', 'line', 'decor', 'driveway', 'park', 'car', 'river', 'island', 'citi', 'well', 'maintain', 'san', 'jacinto', 'extra', 'temporari', 'visitor', 'cute', 'littl', 'situat', 'covet', 'acr', 'bryan', 'access', 'recent', 'purchas', 'mile', 'outsid', 'downtown', 'fort', 'worth', 'happi', 'restor', 'charact', 'built', 'minut', 'drive', 'offer', 'across', 'current', 'rejuven', 'gateway', 'place', 'lake', 'conro', 'famili', 'friend', 'activ', 'nightlif', 'love', 'outdoor', 'space', 'good', 'coupl', 'solo', 'adventur', 'busi', 'travel', 'kid', 'big', 'group', 'rustic', 'countri', 'retreat', 'southeast', 'austin', 'convert', 'modular', 'amen', 'need', 'texa', 'experi', 'less', 'circuit', 'america', 'cota', 'formula', 'one', 'trailer', 'size', 'closet', 'pet', 'alway', 'clean', 'share', 'suppli', 'towel', 'shampoo', 'tcu', 'tcc', 'stockyard', 'first', 'class', 'comfort', 'condo', 'best', 'view', 'bottom', 'deck', 'master', 'patio', 'r', 'pier', 'step', 'away']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1,fileCount):\n",
    "    t = getWordsTSV(i)\n",
    "    for w in t:\n",
    "        if w not in words:\n",
    "            words.append(w)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PS\n",
    "dunno why the stemmer is removing the last 'e' from any word\n",
    "we need to look at the stem function again later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(*words)\n",
    "#words=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reset the words \n",
    "#words =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the vocabulary file from the words \n",
    "wordsCount=0\n",
    "with open('vocabulary.csv','wb') as vfile:\n",
    "    for i in range(0,len(words)):\n",
    "        vfile.write(str(wordsCount).encode())\n",
    "        vfile.write(str('\\t').encode())\n",
    "        vfile.write(str(words[i]).encode())\n",
    "        vfile.write('\\n'.encode())\n",
    "        wordsCount+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['德州农机大学', 'texa', 'amp', 'univers', '北门附近2b1b其中的一间卧室提供日租', '位置便利', '步行进入校区只需要5分钟', '室友为中国男生', '好相处', '日租时间段为2015年12月9日', '适合刚来学校没有车或者不打算买车的同学和老师', '男性']\n"
     ]
    }
   ],
   "source": [
    "#print(cleanData(\"德州农机大学(Texas A&amp;M University)北门附近2b1b其中的一间卧室提供日租。位置便利,步行进入校区只需要5分钟;室友为中国男生,好相处;日租时间段为2015年12月9日~30日。适合刚来学校没有车或者不打算买车的同学和老师(男性)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getID(term):\n",
    "    with open('vocabulary.csv', newline='') as voc:\n",
    "        line = csv.reader(voc, delimiter='\\t')\n",
    "        w = list(map(tuple, line))\n",
    "        #print(w[0][1])\n",
    "        for i in w:\n",
    "            if term == i[1]:\n",
    "                return (i[0])\n",
    "        else:\n",
    "            return None \n",
    "\n",
    "#getID('Hass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes term ID and returns the list of files containing the term\n",
    "def getDocsWithID(tid):\n",
    "    for k in index:\n",
    "        if tid == k:\n",
    "            return(index.get(k))\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 8]"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getDocsWithID(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocswithQuery(q):\n",
    "    docsID = []\n",
    "    for t in cleanData(q):\n",
    "        if (getID(t)) != None: docs = getDocsWithID(int(getID(t)))\n",
    "        if docs != None:\n",
    "            for d in docs:\n",
    "                if d not in docsID: docsID.append(d)\n",
    "\n",
    "    return(docsID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDetails(fid):\n",
    "    with open('data/doc_'+str(fid)+'.tsv') as file:\n",
    "        line = csv.reader(file, delimiter='\\t')\n",
    "        cls = list(map(str, *line))\n",
    "        print(cls[2])\n",
    "        # TODO create a table and list the details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humble\n"
     ]
    }
   ],
   "source": [
    "getDetails(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(getDocsWithID(161))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 8, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print(getDocswithQuery(\"a beautiful house with garden and beach\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bed']"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanData(\"Bed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(getID('bed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from google.cloud import translate\n",
    "translate_client = translate.Client()\n",
    "text = u'Hello, world!'\n",
    "target = 'ru'\n",
    "# Translates some text into Russian\n",
    "translation = translate_client.translate(\n",
    "    text,\n",
    "    target_language=target)\n",
    "\n",
    "print(u'Text: {}'.format(text))\n",
    "print(u'Translation: {}'.format(translation['translatedText']))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [1, 5], 1: [1], 2: [1, 4, 10], 3: [1, 2, 3], 4: [1, 8], 5: [1, 8], 6: [1], 7: [1, 2, 4, 8], 8: [1], 9: [1, 2], 10: [1], 11: [1, 2, 4, 8, 9], 12: [1], 13: [1, 7, 8], 14: [1], 15: [1], 16: [1], 17: [1, 2, 7], 18: [1], 19: [1], 20: [1], 21: [2], 22: [2], 23: [2], 24: [2, 4, 5, 7], 25: [2], 26: [2], 27: [2], 28: [2], 29: [2], 30: [2], 31: [2], 32: [2, 3, 8], 33: [2], 34: [2], 35: [2, 5], 36: [2, 8], 37: [2], 38: [2], 39: [2], 40: [2, 4, 6], 41: [2, 3, 6], 42: [2], 43: [2], 44: [2], 45: [2], 46: [2], 47: [2, 5], 48: [2], 49: [2], 50: [2], 51: [2, 4], 52: [2], 53: [2], 54: [2], 55: [2], 56: [2], 57: [2], 58: [2], 59: [2], 60: [2], 61: [2, 5, 7], 62: [2], 63: [3], 64: [3], 65: [3], 66: [3], 67: [3], 68: [3], 69: [3], 70: [3], 71: [3], 72: [3], 73: [4], 74: [4], 75: [4], 76: [4], 77: [4, 7], 78: [4], 79: [4], 80: [5], 81: [5], 82: [5, 7, 8], 83: [5], 84: [5, 8], 85: [5], 86: [5], 87: [5], 88: [5], 89: [5], 90: [5], 91: [5], 92: [5], 93: [5, 7], 94: [5], 95: [5], 96: [5], 97: [5], 98: [6], 99: [6], 100: [6], 101: [6], 102: [6], 103: [6], 104: [6], 105: [6], 106: [6], 107: [6], 108: [6], 109: [6], 110: [6], 111: [6], 112: [6], 113: [6], 114: [6], 115: [6], 116: [6], 117: [7], 118: [7], 119: [7], 120: [7], 121: [7], 122: [7], 123: [7], 124: [7], 125: [7], 126: [7], 127: [7], 128: [7], 129: [7], 130: [7], 131: [7], 132: [7], 133: [7], 134: [7], 135: [8], 136: [8], 137: [8], 138: [8], 139: [8], 140: [8], 141: [8], 142: [8], 143: [8], 144: [8], 145: [8], 146: [8], 147: [9], 148: [9], 149: [9], 150: [9], 151: [9], 152: [9], 153: [9], 154: [9], 155: [9], 156: [9], 157: [9], 158: [9], 159: [9], 160: [9]}\n"
     ]
    }
   ],
   "source": [
    "# creating index-file by loading vocabulary file and then comparing files with all vocabularys\n",
    "with open('vocabulary.csv', newline='') as file:\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    voc = list(map(tuple, reader))\n",
    "#print(voc)\n",
    "\n",
    "index = {}\n",
    "for i in range(len(voc)):\n",
    "    tempL=[]\n",
    "\n",
    "    for j in range(1,fileCount+1):\n",
    "        temp = getWordsTSV(j)\n",
    "        #print(temp)\n",
    "        if voc[i][1] in temp:\n",
    "            tempL.append(j)\n",
    "            \n",
    "        if len(tempL)!=0: index[i]= tempL\n",
    "            \n",
    "print(index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " I need a beatiful appartment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['need', 'beati', 'appart']"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = input()\n",
    "q_clean = cleanData(q)\n",
    "for w in q_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
