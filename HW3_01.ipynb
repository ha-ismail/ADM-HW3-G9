{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading tsvs and getting cleaned stemmed dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "words = []\n",
    "def getWordsFromTSV(fid):\n",
    "#    with open('data/doc_'+fid+'.tsv') as fd:\n",
    "    with open('doc_1.tsv') as fd:\n",
    "        data = fd.read()\n",
    "        return(data)\n",
    "\n",
    "print(getWordsFromTSV(1))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the given dataset into pandas for processing using only the needed columns\n",
    "data = pd.read_csv('Airbnb_Texas_Rentals.csv', nrows=10, usecols= ['average_rate_per_night', 'bedrooms_count','city', 'date_of_listing', 'description', 'latitude','longitude','title', 'url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a file for every row in data(the given data of airbnb)\n",
    "fileCount = 0\n",
    "for index, r in data.iterrows():\n",
    "    data_temp = data.loc[index:index]\n",
    "    fileCount +=1\n",
    "    data_temp.to_csv('data/doc_'+str(index+1)+'.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE THIS PART \n",
    "# function to extract unique words from tsv file and stor them in (words)\n",
    "# it also remove \\n , puctuations and stopwords\n",
    "# stem words\n",
    "# TODO: create a separate function for the cleaning and stemming stuff, and call it inside the below function\n",
    "# TODO: we need to stem before checking if the words is already there\n",
    "# TODO: create funtion to extract only needed data from a file, then we can use in creating the index\n",
    "\n",
    "def NO_getWordsTSV(fid):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    data = [i.strip('\\n').split('\\t') for i in open('data/doc_'+str(fid)+'.tsv')]\n",
    "    if ((data[0][2] not in words) and (data[0][2] not in stop_words)):words.append(data[0][2].strip())\n",
    "    for w in (tokenizer.tokenize(data[0][4].replace('\\\\n', ' '))):\n",
    "        \n",
    "        if (w not in words and w not in stop_words): words.append(stemmer.stem(w))\n",
    "#words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing the functions\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# list of unique words (terms) to be used to build the vocabulary\n",
    "words = []\n",
    "\n",
    "def cleanData(rawData):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # TODO translatoin\n",
    "    #lang = detect(t4[0])\n",
    "    #print(lang)\n",
    "    \n",
    "    # get words lowercased\n",
    "    t0 = rawData.lower()\n",
    "    # remove puctuations\n",
    "    t1 = tokenizer.tokenize(t0)\n",
    "    #t2Data = stemmer.stem(i for i in t1Data)\n",
    "    #print(len(t1))\n",
    "    # reomve stop words\n",
    "    t2 =[]\n",
    "    t2 = [t1[i] for i in range(0,len(t1)) if t1[i] not in stop_words]\n",
    "    \n",
    "    # stemm words\n",
    "    t3 = [stemmer.stem(t2[i]) for i in range(0, len(t2))]\n",
    "    \n",
    "    # remove nummbers and strings starting with numbers\n",
    "    t4 = [t3[i] for i in range(0, len(t3)) if t3[i][0].isdigit()==False]\n",
    "    \n",
    "\n",
    "    #print(t4)\n",
    "    return(t4)\n",
    "\n",
    "def getWordsTSV(fid):\n",
    "    for i in open('data/doc_'+str(fid)+'.tsv'):\n",
    "        #words.append(cleanData(i[0][4]))\n",
    "        data = [i.strip('\\n').split('\\t') for i in open('data/doc_'+str(fid)+'.tsv')]\n",
    "        for w in (cleanData(data[0][4])):\n",
    "            if w not in words: words.append(w)\n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcom', 'stay', 'privat', 'room', 'queen', 'bed', 'detach', 'bathroom', 'second', 'floor', 'anoth', 'bedroom', 'sofa', 'avail', 'addit', 'guest', 'n10min', 'iah', 'airport', 'nairport', 'pick', 'drop', 'trip', 'stylish', 'fulli', 'remodel', 'home', 'upscal', 'nw', 'alamo', 'height', 'area', 'n', 'namaz', 'locat', 'hous', 'conveni', 'quiet', 'street', 'beauti', 'season', 'tree', 'prestigi', 'neighborhood', 'close', 'loop', 'town', 'nfeatur', 'open', 'plan', 'origin', 'hardwood', 'full', 'independ', 'garden', 'tv', 'sleep', 'neuropean', 'inspir', 'kitchen', 'top', 'line', 'decor', 'driveway', 'park', 'car', 'river', 'island', 'citi', 'na', 'well', 'maintain', 'san', 'jacinto', 'extra', 'temporari', 'visitor', 'cute', 'littl', 'situat', 'covet', 'acr', 'bryan', 'access', 'recent', 'purchas', 'mile', 'outsid', 'downtown', 'fort', 'worth', 'happi', 'restor', 'charact', 'built', 'minut', 'drive', 'offer', 'across', 'current', 'rejuven', 'gateway', 'place', 'lake', 'conro', 'famili', 'friend', 'activ', 'nightlif', 'love', 'outdoor', 'space', 'good', 'coupl', 'solo', 'adventur', 'busi', 'travel', 'kid', 'big', 'group', 'rustic', 'countri', 'retreat', 'southeast', 'austin', 'convert', 'modular', 'amen', 'need', 'texa', 'experi', 'less', 'circuit', 'america', 'cota', 'formula', 'one', 'trailer', 'size', 'closet', 'pet', 'alway', 'clean', 'share', 'suppli', 'towel', 'shampoo', 'tcu', 'tcc', 'stockyard', 'first', 'class', 'comfort', 'condo', 'best', 'view', 'bottom', 'deck', 'master', 'patio', 'r', 'npier', 'step', 'away']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1,fileCount):\n",
    "    getWordsTSV(i)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welcom stay privat room queen bed detach bathroom second floor anoth bedroom sofa avail addit guest n10min iah airport nairport pick drop trip stylish fulli remodel home upscal nw alamo height area n namaz locat hous conveni quiet street beauti season tree prestigi neighborhood close loop town nfeatur open plan origin hardwood full independ garden tv sleep neuropean inspir kitchen top line decor driveway park car river island citi na well maintain san jacinto extra temporari visitor cute littl situat covet acr bryan access recent purchas mile outsid downtown fort worth happi restor charact built minut drive offer across current rejuven gateway place lake conro famili friend activ nightlif love outdoor space good coupl solo adventur busi travel kid big group rustic countri retreat southeast austin convert modular amen need texa experi less circuit america cota formula one trailer size closet pet alway clean share suppli towel shampoo tcu tcc stockyard first class comfort condo best view bottom deck master patio r npier step away\n"
     ]
    }
   ],
   "source": [
    "print(*words)\n",
    "#words=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reset the words \n",
    "#words =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the vocabulary file from the words \n",
    "wordsCount=0\n",
    "with open('vocabulary.csv','wb') as vfile:\n",
    "    for i in range(0,len(words)):\n",
    "        vfile.write(str(wordsCount).encode())\n",
    "        vfile.write(str('\\t').encode())\n",
    "        vfile.write(str(words[i]).encode())\n",
    "        vfile.write('\\n'.encode())\n",
    "        wordsCount+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Humble'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
