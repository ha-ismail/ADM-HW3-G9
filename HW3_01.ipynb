{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading tsvs and getting cleaned stemmed dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "def getWordsFromTSV(fid):\n",
    "#    with open('data/doc_'+fid+'.tsv') as fd:\n",
    "    with open('doc_1.tsv') as fd:\n",
    "        data = fd.read()\n",
    "        return(data)\n",
    "\n",
    "print(getWordsFromTSV(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the given dataset into pandas for processing using only the needed columns\n",
    "data = pd.read_csv('Airbnb_Texas_Rentals.csv', nrows=10, usecols= ['average_rate_per_night', 'bedrooms_count','city', 'date_of_listing', 'description', 'latitude','longitude','title', 'url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a file for every row in data(the given data of airbnb)\n",
    "for index, r in data.iterrows():\n",
    "    data_temp = data.loc[index:index]\n",
    "    data_temp.to_csv('data/doc_'+str(index+1)+'.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "#cache_english_stopwords=stopwords.words('english')\n",
    "#cache_en_tweet_stopwords=stopwords.words('english_tweet')\n",
    "\n",
    "# function to extract unique words from tsv file and stor them in (words)\n",
    "# it also remove \\n , puctuations and stopwords\n",
    "# stem words\n",
    "# TODO: create a separate function for the cleaning and stemming stuff, and call it inside the below function\n",
    "# TODO: we need to stem before checking if the words is already there\n",
    "# TODO: create funtion to extract only needed data from a file, then we can use in creating the index\n",
    "\n",
    "def getWordsTSV(fid):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    data = [i.strip('\\n').split('\\t') for i in open('data/doc_'+str(fid)+'.tsv')]\n",
    "    if ((data[0][2] not in words) and (data[0][2] not in stop_words)):words.append(data[0][2].strip())\n",
    "    for w in (tokenizer.tokenize(data[0][4].replace('\\\\n', ' '))):\n",
    "        \n",
    "        if (w not in words and w not in stop_words): words.append(stemmer.stem(w))\n",
    "#words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n",
      "['ein', 'welcom', 'stay', 'privat', 'room', 'queen', 'bed', 'detach', 'privat', 'bathroom', 'second', 'floor', 'anoth', 'privat', 'bedroom', 'sofa', 'bed', 'avail', 'addit', 'guest', 'addit', 'guest', 'iah']\n"
     ]
    }
   ],
   "source": [
    "# TEMP for organizing the functions\n",
    "from langdetect import detect\n",
    "def cleanData(rawData):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # TODO translatoin\n",
    "    lang = detect(t4[0])\n",
    "    print(lang)\n",
    "    \n",
    "    # get words lowercased\n",
    "    t0 = rawData.lower()\n",
    "    # remove puctuations\n",
    "    t1 = tokenizer.tokenize(t0)\n",
    "    #t2Data = stemmer.stem(i for i in t1Data)\n",
    "    #print(len(t1))\n",
    "    # reomve stop words\n",
    "    t2 =[]\n",
    "    t2 = [t1[i] for i in range(0,len(t1)) if t1[i] not in stop_words]\n",
    "    \n",
    "    # stemm words\n",
    "    t3 = [stemmer.stem(t2[i]) for i in range(0, len(t2))]\n",
    "    \n",
    "    # remove nummbers and strings starting with numbers\n",
    "    t4 = [t3[i] for i in range(0, len(t3)) if t3[i][0].isdigit()==False]\n",
    "    \n",
    "    \n",
    "    print(t4)\n",
    "\n",
    "cleanData(\"Ein Welcome to stay in private room with queen bed and detached private bathroom on the second floor. Another private bedroom with sofa bed is available for additional guests. 10$ for an additional guest.\\n10min from IAH \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dsasd', 'asdasd', 'nasd', 'asd']"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TESTING\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize(\"dsasd,asdasd\\\\nasd  asd \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,index):\n",
    "    getWordsTSV(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humble welcom stay privat room queen bed detach privat bathroom second floor anoth privat bedroom sofa avail addit guest 10 addit 10min iah airport airport pick drop avail trip San Antonio stylish fulli remodel home upscal nw alamo height area amaz locat hous conveni locat quiet street beauti season tree prestigi neighborhood close 281 410 loop town featur open plan origin hardwood floor 3 bedroom full bathroom independ garden tv sleep 2 european inspir kitchen top line decor driveway park 4 car Houston river hous island citi a well maintain hous san jacinto extra temporari visitor Bryan privat cute littl situat covet garden acr the privat access privat Fort Worth welcom origin 1920 we recent purchas 1922 mile outsid downtown fort worth happi restor origin charact built you 5 minut drive offer across current rejuven gateway park Conroe my place lake famili friend activ nightlif you love outdoor space my good coupl solo adventur busi travel famili kid big group Cedar Creek rustic countri retreat 8 acr southeast austin convert modular amen need offer texa experi less 15 mile austin circuit america cota formula one trailer park this beauti size closet we pet hous alway clean the share suppli towel shampoo avail we mile downtown tcu tcc stockyard\n"
     ]
    }
   ],
   "source": [
    "print(*words)\n",
    "#words=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reset the words \n",
    "words =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the vocabulary file from the words \n",
    "wordsCount=0\n",
    "with open('vocabulary.csv','wb') as vfile:\n",
    "    for i in range(0,len(words)):\n",
    "        vfile.write(str(wordsCount).encode())\n",
    "        vfile.write(str('\\t').encode())\n",
    "        vfile.write(str(words[i]).encode())\n",
    "        vfile.write('\\n'.encode())\n",
    "        wordsCount+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Humble'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
