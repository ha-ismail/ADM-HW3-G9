{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading tsvs and getting cleaned stemmed dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "words = []\n",
    "def getWordsFromTSV(fid):\n",
    "#    with open('data/doc_'+fid+'.tsv') as fd:\n",
    "    with open('doc_1.tsv') as fd:\n",
    "        data = fd.read()\n",
    "        return(data)\n",
    "\n",
    "print(getWordsFromTSV(1))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the given dataset into pandas for processing using only the needed columns\n",
    "data = pd.read_csv('Airbnb_Texas_Rentals.csv', nrows=10, usecols= ['average_rate_per_night', 'bedrooms_count','city', 'date_of_listing', 'description', 'latitude','longitude','title', 'url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a file for every row in data(the given data of airbnb)\n",
    "fileCount = 0\n",
    "for index, r in data.iterrows():\n",
    "    data_temp = data.loc[index:index]\n",
    "    fileCount +=1\n",
    "    data_temp.to_csv('data/doc_'+str(index+1)+'.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE THIS PART \n",
    "# function to extract unique words from tsv file and stor them in (words)\n",
    "# it also remove \\n , puctuations and stopwords\n",
    "# stem words\n",
    "# TODO: create a separate function for the cleaning and stemming stuff, and call it inside the below function\n",
    "# TODO: we need to stem before checking if the words is already there\n",
    "# TODO: create funtion to extract only needed data from a file, then we can use in creating the index\n",
    "\n",
    "def NO_getWordsTSV(fid):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    data = [i.strip('\\n').split('\\t') for i in open('data/doc_'+str(fid)+'.tsv')]\n",
    "    if ((data[0][2] not in words) and (data[0][2] not in stop_words)):words.append(data[0][2].strip())\n",
    "    for w in (tokenizer.tokenize(data[0][4].replace('\\\\n', ' '))):\n",
    "        \n",
    "        if (w not in words and w not in stop_words): words.append(stemmer.stem(w))\n",
    "#words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing the functions\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from googletrans import Translator\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "translator = Translator()\n",
    "\n",
    "# list of unique words (terms) to be used to build the vocabulary\n",
    "words = []\n",
    "\n",
    "def cleanData(rawData):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # TODO translatoin\n",
    "    #lang = detect(t4[0])\n",
    "    #print(lang)\n",
    "    #rawData = translator.translate(rawData, dest='en')\n",
    "    \n",
    "    # get words lowercased\n",
    "    t0 = rawData.lower()\n",
    "    # remove puctuations\n",
    "    t1 = tokenizer.tokenize(t0)\n",
    "    #t2Data = stemmer.stem(i for i in t1Data)\n",
    "    #print(len(t1))\n",
    "    # reomve stop words\n",
    "    t2 =[]\n",
    "    t2 = [t1[i] for i in range(0,len(t1)) if t1[i] not in stop_words]\n",
    "    \n",
    "    # stemm words\n",
    "    t3 = [stemmer.stem(t2[i]) for i in range(0, len(t2))]\n",
    "    \n",
    "    # remove nummbers and strings starting with numbers\n",
    "    t4 = [t3[i] for i in range(0, len(t3)) if t3[i][0].isdigit()==False]\n",
    "    \n",
    "\n",
    "    #print(t4)\n",
    "    return(t4)\n",
    "\n",
    "def getWordsTSV(fid):\n",
    "    fileWords=[]\n",
    "    for i in open('data/doc_'+str(fid)+'.tsv'):\n",
    "        #words.append(cleanData(i[0][4]))\n",
    "        data = [i.strip('\\n').split('\\t') for i in open('data/doc_'+str(fid)+'.tsv')]\n",
    "        for w in (cleanData(data[0][4].replace('\\\\n', ' '))):\n",
    "            if w not in fileWords: fileWords.append(w)\n",
    "    return(fileWords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welcom stay privat room queen bed detach bathroom second floor anoth bedroom sofa avail addit guest iah airport pick drop trip\n"
     ]
    }
   ],
   "source": [
    "print(*getWordsTSV(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welcom stay privat room queen bed detach bathroom second floor anoth bedroom sofa avail addit guest iah airport pick drop trip stylish fulli remodel home upscal nw alamo height area amaz locat hous conveni quiet street beauti season tree prestigi neighborhood close loop town featur open plan origin hardwood full independ garden tv sleep european inspir kitchen top line decor driveway park car river island citi well maintain san jacinto extra temporari visitor cute littl situat covet acr bryan access recent purchas mile outsid downtown fort worth happi restor charact built minut drive offer across current rejuven gateway place lake conro famili friend activ nightlif love outdoor space good coupl solo adventur busi travel kid big group rustic countri retreat southeast austin convert modular amen need texa experi less circuit america cota formula one trailer size closet pet alway clean share suppli towel shampoo tcu tcc stockyard first class comfort condo best view bottom deck master patio r pier step away\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1,fileCount):\n",
    "    t = getWordsTSV(i)\n",
    "    for w in t:\n",
    "        if w not in words:\n",
    "            words.append(w)\n",
    "print(*words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PS\n",
    "dunno why the stemmer is removing the last 'e' from any word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*words)\n",
    "#words=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reset the words \n",
    "#words =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the vocabulary file from the words \n",
    "wordsCount=0\n",
    "with open('vocabulary.csv','wb') as vfile:\n",
    "    for i in range(0,len(words)):\n",
    "        vfile.write(str(wordsCount).encode())\n",
    "        vfile.write(str('\\t').encode())\n",
    "        vfile.write(str(words[i]).encode())\n",
    "        vfile.write('\\n'.encode())\n",
    "        wordsCount+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['德州农机大学', 'texa', 'amp', 'univers', '北门附近2b1b其中的一间卧室提供日租', '位置便利', '步行进入校区只需要5分钟', '室友为中国男生', '好相处', '日租时间段为2015年12月9日', '适合刚来学校没有车或者不打算买车的同学和老师', '男性']\n"
     ]
    }
   ],
   "source": [
    "print(cleanData(\"德州农机大学(Texas A&amp;M University)北门附近2b1b其中的一间卧室提供日租。位置便利,步行进入校区只需要5分钟;室友为中国男生,好相处;日租时间段为2015年12月9日~30日。适合刚来学校没有车或者不打算买车的同学和老师(男性)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import translate\n",
    "translate_client = translate.Client()\n",
    "text = u'Hello, world!'\n",
    "target = 'ru'\n",
    "# Translates some text into Russian\n",
    "translation = translate_client.translate(\n",
    "    text,\n",
    "    target_language=target)\n",
    "\n",
    "print(u'Text: {}'.format(text))\n",
    "print(u'Translation: {}'.format(translation['translatedText']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, fileCount):\n",
    "    for j in ran"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
