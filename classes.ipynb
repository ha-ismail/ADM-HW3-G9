{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes Markdown File\n",
    "Here we are explaining the classes and the functions we created.\n",
    "\n",
    "##### Importing the moldules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import csv\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from translate import Translator\n",
    "from langdetect import detect\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "from pathlib import Path\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy import spatial\n",
    "import heapq as hq\n",
    "import geopy\n",
    "import geopy.distance\n",
    "\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Functions\n",
    "\n",
    "Functions used by various classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cleanData()\n",
    "This function takes text data and cleans it by converting to lower case, remove punctuations and stop words and then stems the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other functions \n",
    "\n",
    "def cleanData(rawData, lang='english'):\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        stop_words = set(stopwords.words(lang))\n",
    "\n",
    "        # get words lowercased\n",
    "        t0 = rawData.lower()\n",
    "        # remove puctuations\n",
    "        t1 = tokenizer.tokenize(t0)\n",
    "\n",
    "        # reomve stop words\n",
    "        t2 =[]\n",
    "        t2 = [t1[i] for i in range(0,len(t1)) if t1[i] not in stop_words]\n",
    "\n",
    "        # stemm words\n",
    "        t3 = [stemmer.stem(t2[i]) for i in range(0, len(t2))]\n",
    "\n",
    "        # remove nummbers and strings starting with numbers\n",
    "        t4 = [t3[i] for i in range(0, len(t3)) if t3[i][0].isdigit()==False]\n",
    "\n",
    "        return(t4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getCosineSimi()\n",
    "This function takes two vectors of double and returns the cosine similarity between them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the cosine similarity between two given vectors\n",
    "\n",
    "def getCosineSimi(v1,v2):\n",
    "    return (1 - spatial.distance.cosine(v1, v2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saveData()\n",
    "A function to save a dataframe to a file on the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def saveData(data, file):\n",
    "    feather.write_dataframe(data, file+'.feather')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AirbnbData Class\n",
    "This class represents the whole dataframe we work on and will be used by all search engines as we will see later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirbnbData:\n",
    "    def __init__(self, csvFile, dataName='AirbnbData Texas', buildData = False):\n",
    "        # initiate class attributes\n",
    "        self.name = dataName\n",
    "        self.csvFile = csvFile\n",
    "        if buildData:\n",
    "            self.data = pd.read_csv(self.csvFile, usecols= ['average_rate_per_night', \n",
    "                                                        'bedrooms_count','city', \n",
    "                                                        'date_of_listing', \n",
    "                                                        'description', \n",
    "                                                        'latitude',\n",
    "                                                        'longitude',\n",
    "                                                        'title', \n",
    "                                                        'url'])\n",
    "        self.fileCount = 0\n",
    "        self.lang = \"English\"\n",
    "        self.dataFolder = \"data\"\n",
    "        self.vocabularyFile = \"vocabulary.csv\"\n",
    "        self.words = []\n",
    "        self.vocabulary = self.getVocabulary()\n",
    "        self.voc = self.vocabulary\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### readData()\n",
    "A function to read saved data from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # this function can read a dataframe file and assign to Airbnb object if it was initiated without reading data\n",
    "    def readData(self, file):\n",
    "        self.data = feather.read_dataframe(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### present()\n",
    "A function used only for showing messages to users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # function to show messages when running some functions\n",
    "    def present(self, outHtml):\n",
    "        styles = open(\"style/style.css\", \"r\").read()\n",
    "        outputHTML = '<style>%s</style>' % styles\n",
    "        outputHTML += outHtml\n",
    "        display(HTML(outputHTML))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean()\n",
    "#### removeDuplicates()\n",
    "\n",
    "The first function is to clean data from any null value in the columns wanted from the dataframe.\n",
    "\n",
    "The second function we created because during our process we found some entries in the data which have similar values (title, description and other information) but they are different entries (we supposed that the owner of property posted twice or more about his property for some reason) so using this function we can remove the duplicates, __But we didnt go with this__ and we kept the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # cleaning the data\n",
    "    def clean(self):\n",
    "        self.data = self.data.dropna(how='any',axis=0, subset=['average_rate_per_night', \n",
    "                                                               'bedrooms_count','city', \n",
    "                                                               'description', 'latitude',\n",
    "                                                               'longitude',\n",
    "                                                               'title'])\n",
    "        self.fileCount = len(self.data)\n",
    "        self.data.index = range(len(self.data.index))\n",
    "\n",
    "        hhtm = '<div class=\"h_msg\">'+self.name+' data has been cleaned from null values sir!</div>'\n",
    "        \n",
    "        self.present(hhtm)\n",
    "    \n",
    "    def removeDuplicates(self):\n",
    "        #\n",
    "        hhtm = '<div class=\"h_msg\">Duplicates data have been removed from '+self.name+' sir!</div>'\n",
    "        \n",
    "        self.present(hhtm)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### translate()\n",
    "A function to translate title and description into the desired language (here we transalted into English).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def translate(self,lang=\"English\"):\n",
    "        # we have noticed some text values in language other than english, so not to lose them we are going to translate them into English \n",
    "\n",
    "        translator= Translator(to_lang=lang)\n",
    "        for index, row in self.data.iterrows():\n",
    "            try:\n",
    "                if detect(row['description']) != 'en' and len(row['description']) >3:\n",
    "                    row['description'] = translator.translate(row['description'])\n",
    "                if detect(row['title']) != 'en' and len(row['title']) >3:\n",
    "                    row['title'] = translator.translate(row['title'])\n",
    "            except :\n",
    "                print(\"Error: in detecting language with message\")\n",
    "                print(index)\n",
    "                self.data.drop([index])\n",
    "        # reset the index to remove gaps in index\n",
    "        self.data.index = range(len(self.data.index))\n",
    "        self.fileCount = len(self.data)\n",
    "        hhtm = '<div class=\"h_msg\">'+self.name+' has been translated into'+lang+'sir!</div>'\n",
    "        \n",
    "        self.present(hhtm)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### createTSV()\n",
    "A function to create TSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def createTSVs(self, folder=\"data\"):\n",
    "        self.fileCount = 0\n",
    "        for index, r in self.data.iterrows():\n",
    "            data_temp = self.data.loc[index:index]\n",
    "            self.fileCount +=1\n",
    "            data_temp.to_csv(self.dataFolder+'/doc_'+str(index+1)+'.tsv', sep='\\t', index=False, header=False)\n",
    "        hhtm = '<div class=\"h_msg\">TSV files for '+self.name+' have been created under '+self.dataFolder+' folder, sir!</div>'\n",
    "        \n",
    "        self.present(hhtm)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getUniqueTerms()\n",
    "This function will return the unique terms in our data (title and description) by calling a function from class Property for each entry in data.\n",
    "This is used to build the vocabulary file and the index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # get all unique words in all documents and store them in a list words[]\n",
    "    def getUniqueTerms(self):\n",
    "        for i in range(1,self.fileCount):\n",
    "            p = Property(i)\n",
    "            terms = p.getUniqueTerms()\n",
    "            for word in terms:\n",
    "                if word not in self.words:\n",
    "                    self.words.append(word)\n",
    "        return(self.words)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### buildVocabulary()\n",
    "A function to build the vocabulary dictionary and save it to a file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # build the vocabulary\n",
    "    def buildVocabulary(self, fileName='vocabulary.csv'):\n",
    "        self.vocabularyFile = str(fileName)\n",
    "        self.words = self.getUniqueTerms()\n",
    "        wordID=0\n",
    "        with open(self.vocabularyFile,'wb') as vfile:\n",
    "            for i in range(0,len(self.words)):\n",
    "                vfile.write(str(wordID).encode())\n",
    "                vfile.write(str('\\t').encode())\n",
    "                vfile.write(str(self.words[i]).encode())\n",
    "                vfile.write('\\n'.encode())\n",
    "                wordID+=1\n",
    "        vfile.close()\n",
    "        \n",
    "        hhtm = '<div class=\"h_msg\">Vocabulary file for '+self.name+' has been created in '+self.vocFile+', sir!</div>' \n",
    "        self.present(hhtm)\n",
    "\n",
    "    # a function to read vocabulary from file\n",
    "    def getVocabulary(self):\n",
    "        with open(self.vocabularyFile, newline='') as file:\n",
    "            reader = csv.reader(file, delimiter='\\t')\n",
    "            self.vocabulary = np.array(list(map(tuple, reader)))\n",
    "        file.close()\n",
    "        return(self.vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getID() getTerm()\n",
    "Functions to get the term ID by passing the term, and to get the term by passing its ID (from the vocabulary file).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # function to get ID of a given term (this one uses the voc array instead of reading from file)\n",
    "    def getID(self,term):\n",
    "        for row in self.vocabulary:\n",
    "            if row[1] == term:\n",
    "                return(row[0])\n",
    "        return(-1)\n",
    "    # function to get Term for a given term ID\n",
    "    def getTerm(self,tid):\n",
    "        for row in self.vocabulary:\n",
    "            if row[0] == tid:\n",
    "                return(row[1])\n",
    "        return('')\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### showMap()\n",
    "\n",
    "Function used for the bonus task to show the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def showMap(self, lat, long):\n",
    "        m = folium.Map(location=[lat,long])\n",
    "        self.data.index = range(len(self.data.index))\n",
    "        folium.Circle(\n",
    "            location=[lat,long],\n",
    "            radius=dis*1000,\n",
    "            color='#3186cc',\n",
    "            fill=True,\n",
    "            fill_color='#3186cc'\n",
    "        ).add_to(m)\n",
    "\n",
    "        for i in range(len(self.data)):\n",
    "            coor=(self.data['latitude'][i], self.data['longitude'][i])\n",
    "            if geopy.distance.distance(distanceFrom, coor).km<=dis:\n",
    "                folium.Marker(coor, popup=str(self.data['title'][i])).add_to(m)\n",
    "\n",
    "        return(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property Class\n",
    "\n",
    "This class represents the entrys (house listsings) in Airbnb data.\n",
    "The object of this class takes the file ID as parameter and reads the TSV (with this ID) and stores the data into its attributes, so it can used later by objects from other classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for Property\n",
    "class Property:\n",
    "    def __init__(self, fileId, fromFile = False, path = 'data'):\n",
    "        self.fileId = fileId\n",
    "        self.terms = []\n",
    "        self.uniqueTerms = []\n",
    "        self.path = path\n",
    "        self.title = ''\n",
    "        self.description = ''\n",
    "        self.rooms = 0\n",
    "        self.url = 0\n",
    "        self.lat = 0\n",
    "        self.long = 0\n",
    "        self.city = 0\n",
    "        if fromFile == True:\n",
    "            self.getFromTSV()\n",
    "        \n",
    "    def getFromTSV(self):    \n",
    "        data = [i.strip('\\n').split('\\t') for i in open(self.path+'/doc_'+str(self.fileId)+'.tsv',encoding=\"utf8\")]\n",
    "        self.title = data[0][7].replace('\\\\n', ' ')\n",
    "        self.description = data[0][4].replace('\\\\n', ' ')\n",
    "        for w in (cleanData(self.title)):\n",
    "            self.terms.append(w)\n",
    "            if w not in self.uniqueTerms: self.uniqueTerms.append(w)\n",
    "        for w in (cleanData(self.description)):\n",
    "            self.terms.append(w)\n",
    "            if w not in self.uniqueTerms: self.uniqueTerms.append(w) \n",
    "        \n",
    "        self.price = float(str(data[0][0]).replace('$','')) \n",
    "        self.rooms = float(data[0][1].replace('Studio','1'))\n",
    "        self.url = data[0][8]\n",
    "        self.lat = data[0][5]\n",
    "        self.long = data[0][6]\n",
    "        self.city = data[0][2]\n",
    "        \n",
    "        \n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getUniqueTerms()\n",
    "Returns the unique terms in the description and title of a file.\n",
    "\n",
    "#### getDetailsSimple()\n",
    "Returns the details of the property object (used by search engine in printing the results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "        \n",
    "    # function to get unique words from a TSV file given its ID\n",
    "    def getUniqueTerms(self):\n",
    "            data = [i.strip('\\n').split('\\t') for i in open(self.path+'/doc_'+str(self.fileId)+'.tsv',encoding=\"utf8\")]\n",
    "            for w in (cleanData(data[0][4].replace('\\\\n', ' '))):\n",
    "                if w not in self.uniqueTerms: self.uniqueTerms.append(w)\n",
    "            for w in (cleanData(data[0][7].replace('\\\\n', ' '))):\n",
    "                if w not in self.uniqueTerms: self.uniqueTerms.append(w)\n",
    "            \n",
    "            return(list(set(self.uniqueTerms)))\n",
    "     \n",
    "        \n",
    "    # funciton retuns the details of a file given file id\n",
    "\n",
    "    def getDetailsSimple(self):\n",
    "        return([self.fileId, self.title, self.description, self.city, self.url])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HoohleSimple Class (Step 3.1)\n",
    "\n",
    "This class represents the first search engine. \n",
    "The object of this class takes the data (airbnbData) object which will be searched and boolean value if it needs to create the index or read it from file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoohleSimple:\n",
    "    def __init__(self, airbnb, buildIndex = False):\n",
    "        self.data = airbnb\n",
    "        self.index = {}\n",
    "        #if buildIndex:\n",
    "        for f in range(1,self.data.fileCount+1):\n",
    "            p= Property(f,True)\n",
    "            for t in p.uniqueTerms:\n",
    "                termID = self.data.getID(t)\n",
    "                if termID in self.index:\n",
    "                    self.index[termID].append(f)\n",
    "                else:\n",
    "                    self.index[termID]=[f]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saveIndex()\n",
    "#### readIndex()\n",
    "\n",
    "Two functions to save and read index to/from a file on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # two functions to save and read index to/from a file\n",
    "    def saveIndex(self, file):\n",
    "        deepdish.io.save(file, self.index)\n",
    "    def readIndex(self, file):\n",
    "        self.index = deepdish.io.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getDocsByID()\n",
    "#### getDocsByTerm()\n",
    "\n",
    "Functions to retrieve documents containing a specific term (either by passing the term ID or the term itself cleaned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "    # get list of docs contains a given term ID\n",
    "    def getDocsByID(self, termId):\n",
    "        return (self.index[str(termId)])\n",
    "    # get list of docs contains a given term \n",
    "    def getDocsByTerm(self, term):\n",
    "        return (self.index[str(self.data.getID(term))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getDocsByQuery()\n",
    "\n",
    "The main function of the class, which takes a query and returns the documents containing all the terms in the cleaned and stemmed text of the query.\n",
    "It uses the cleanData function to clean, stem text and return the terms. Then it takes the intersections of the lists of the documents containing all terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Search Engine 1\n",
    "    def getDocsByQuery(self, q):\n",
    "        ts = cleanData(q)\n",
    "        docsIDs = self.getDocsByTerm(ts[0])\n",
    "        \n",
    "        if docsIDs == None : return []\n",
    "        if len(ts)>1:\n",
    "            for t in ts[1:]:\n",
    "                tempDocs = self.getDocsByTerm(t)\n",
    "                if tempDocs != None:\n",
    "                        docsIDs = set(docsIDs).intersection(tempDocs)\n",
    "                else: return None\n",
    "        return(list(docsIDs))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### printresult()\n",
    "#### printResults()\n",
    "\n",
    "The presentaion functions, the latter take the query text, call the __getDocsByQuery__ to get the resulting documents, and then initiate a __Property__ object for each document and call the first function __printResult()__ to present it, using HTML display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def printResult(self, property):\n",
    "        rhtm ='<div class=\"h_result\"><div class=\"h_title\"><a href=\"'+property[4]+'\">'+property[1]+'</a></div>'\n",
    "        rhtm +='<div class=\"h_link\"><a href=\"'+property[4]+'\">'+property[4]+'</a></div>'\n",
    "        rhtm +='<div class=\"h_city\">'+str(property[3])+'</div>'\n",
    "        rhtm +='<div class=\"h_disc\">'+property[2].replace('\\n', '')[:300]+'... </div>'\n",
    "        rhtm +='</div>'\n",
    "        return (rhtm)\n",
    "    \n",
    "    def printResults(self, query):\n",
    "        docs = self.getDocsByQuery(query)\n",
    "        styles = open(\"style/style.css\", \"r\").read()\n",
    "        hhtm = '<style>%s</style>' % styles     \n",
    "        hhtm +='<div class=\"Hoohle\" ><div class=\"h_results\">'\n",
    "        if docs != None:\n",
    "            for doc in docs:\n",
    "                p = Property(doc, True)\n",
    "                hhtm += self.printResult(p.getDetailsSimple())\n",
    "        else:\n",
    "            hhtm +='<div class=\"h_sorry\">Sorry! No results found for your request!</div>'\n",
    "        hhtm += '</div></div>'\n",
    "        display(HTML(hhtm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HoohleTFIDF Class (Step 3.2)\n",
    "\n",
    "This class represents the second search engine, and it inherits the first search engine class __(HoohleSimple)__. \n",
    "The object of this class takes the data (airbnbData) object which will be searched and boolean value if it needs to create the index or read it from file.\n",
    "This class builds a new index __indexTFIDF__ which contains each term with the a list of the documents containing it and the score of this term inside the document using __Tf-IDF__ which is calculated by other functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoohleTFIDF(HoohleSimple):\n",
    "    def __init__(self, airbnb, buildIndex=False):\n",
    "        HoohleSimple.__init__(self, airbnb, buildIndex)\n",
    "        \n",
    "        self.wordsAll = []\n",
    "        \n",
    "        # Create the new inverted index\n",
    "        #print(\"building the inverted index, this could take few minutes.. please wait!\")\n",
    "        self.indexTFIDF = {}\n",
    "        #if buildIndex:\n",
    "        for termID in list(self.index.keys()):\n",
    "            self.indexTFIDF[termID]=[]\n",
    "            for docID in self.index[termID]:\n",
    "                self.indexTFIDF[termID].append(self.get_TFIDF(docID, termID))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saveIndex()\n",
    "#### readIndex()\n",
    "\n",
    "Functions to save/read indexTFIDF to/from a file on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # two functions to save and read index to/from a file\n",
    "    def saveIndex(self, file):\n",
    "        deepdish.io.save(file, self.indexTFIDF)\n",
    "    def readIndex(self, file):\n",
    "        self.indexTFIDF = deepdish.io.load(file)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_TF()\n",
    "Is used to get term frequency in a document.\n",
    "\n",
    "#### get_IDF()\n",
    "Is used to get the __IDF__ for a term in all documents, using the IDF formula. \n",
    "\n",
    "#### get_TFIDF()\n",
    "Returns the __TF-IDF__ for a given term in a given document using the previous two functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # for a given (file id, term), returns the term frequency in this file\n",
    "    def get_TF(self,fid, term):\n",
    "        p = Property(fid, True)\n",
    "        words = p.terms\n",
    "        return (words.count(term)/len(words))\n",
    "\n",
    "    # returns the IDF for a given term in the whole data we have\n",
    "    def get_IDF(self, term):\n",
    "        return (math.log(self.data.fileCount/len(self.index[self.data.getID(term)])))\n",
    "    \n",
    "    # returns the TFIDF for a given (file id, term)\n",
    "    def get_TFIDF(self, fid, termID):\n",
    "        #getting TF for term in document with document id = fid and multiply it with the IDF of the term\n",
    "        term = self.data.vocabulary[int(termID)][1]\n",
    "        return ([fid, self.get_TF(fid, term) * self.get_IDF(term)])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getTFIDF_query()\n",
    "A function to get the TF-IDF for the user query text, by first cleaning the query text and then calling the previous functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # function to get the tfidf for a given query\n",
    "    def getTFIDF_query(self, q):\n",
    "        terms = cleanData(q)\n",
    "        return([(terms.count(w))*math.log(self.data.fileCount+1/(1+len(self.index[self.data.getID(w)]))) for w in list(set(terms))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getTFIDF_vector()\n",
    "This function takes a given query and document id, and calculates the __TF-IDF__ for this query in the document based on all other documents. It returns a vector of TFIDF for each unique term in the cleaned text from query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # function to get the tfidf vector for a given query in a document \n",
    "    def getTFIDF_vector(self, doc, terms):\n",
    "        v=[]\n",
    "        p = Property(doc, True)\n",
    "        \n",
    "        for term in list(set(cleanData(terms))):\n",
    "            if term not in p.uniqueTerms: v.append(0)\n",
    "            else:\n",
    "                l = self.indexTFIDF[str(self.data.getID(term))]\n",
    "                for i in l:\n",
    "                    if i[0]==doc:\n",
    "                        v.append(i[1])\n",
    "        return(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getNdocs()\n",
    "This function returns terms with specific repetition in a text.\n",
    "We used this function to expedite the processing of retrieving the results of the search when it is less than __k__, we will explain this in more details later when talking about getting the result documents.\n",
    "It takes two parameters (the first is the list and the second is th threshold) and it returns a list with items which have repitions equal or more than the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # function to get items with repitition more or equal to a threshold from a given list\n",
    "    def getNdocs(self, l, n):\n",
    "        docs = []\n",
    "        docs = list(filter(lambda x:l.count(x)>=n, l))\n",
    "        return(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### completeToK()\n",
    "This function is triggerd only if the query resulted list of documents __n__ less than the threshold __k__. \n",
    "It searchs in the rest of documents which contain any term of the query (execluding the resulted __n__ documents) for the best matches with the query (docs with highest simialrity score with query).\n",
    "The Idea: Realizing that the search will be slow due to the number of documents and process behind calculating the simialrity socre, we implemented an approach to reduce the number of documents we look in:\n",
    "logically the documents with highest similarty socre with a query (of __T__ terms) should contain the (__T - 1__ terms).\n",
    "So for a query with __T__ terms, when we get __n__ documents (less than __k__ ) we first take the list of documents which contain any term of the query:\n",
    "```python\n",
    "for term in cleanData(qq):\n",
    "            l.extend(self.index[self.data.getID(term)])\n",
    "```\n",
    " In this list we search in the documents containing __T-1__ terms of the query, which logiacally will have the highest similarity scores:\n",
    "```python\n",
    "if len(l)>k:\n",
    "            l = self.getNdocs(l, len(cleanData(qq))-1)\n",
    "\n",
    "        for d in list(set(l)):\n",
    "\n",
    "            if d not in r:\n",
    "                cosS = self.getCosineSimi(qTFIDF, self.getTFIDF_vector(d,qq))\n",
    "                if cosS != 0:\n",
    "                    rk.append([d, cosS])\n",
    "```\n",
    "and finally use a __heap__ we return the __k-n__ documents with top similarity scores:\n",
    "```python\n",
    "\n",
    "        return(hq.nlargest(k-len(r), rk, key=lambda x:x[1]))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # function (in case we got results less than K) this function will complete the resulted documents to k from the heap\n",
    "    def completeToK(self, qq, r,k):\n",
    "        if len(r)>k: return None\n",
    "        rk = []\n",
    "        qTFIDF = self.getTFIDF_query(qq)\n",
    "        l = []\n",
    "        for term in cleanData(qq):\n",
    "            l.extend(self.index[self.data.getID(term)])\n",
    "\n",
    "        if len(l)>k:\n",
    "            l = self.getNdocs(l, len(cleanData(qq))-1)\n",
    "\n",
    "        for d in list(set(l)):\n",
    "\n",
    "            if d not in r:\n",
    "                cosS = self.getCosineSimi(qTFIDF, self.getTFIDF_vector(d,qq))\n",
    "                if cosS != 0:\n",
    "                    rk.append([d, cosS])\n",
    "\n",
    "        return(hq.nlargest(k-len(r), rk, key=lambda x:x[1]))\n",
    "\n",
    "    # funciton retuns the details of a file given file id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getDetails()\n",
    "This function is similar to the same function in __Property__ class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def getDetails(self, fid, q):\n",
    "        detlist =[]\n",
    "        smlrty = self.getCosineSimi(self.getTFIDF_query(q), self.getTFIDF_vector(fid,q))\n",
    "        p = Property(fid, True)\n",
    "        \n",
    "        return([p.title, p.description, p.city, p.url, smlrty])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### printResult()\n",
    "#### printResults()\n",
    "\n",
    "Same functions as in __HoohleSimple__ class, but they take into account to show the __similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    def printResult(self, r):\n",
    "        rhtm ='<div class=\"h_result\"><div class=\"h_title\"><a href=\"'+str(r[3])+'\">'+str(r[0])+'</a></div>'\n",
    "        rhtm +='<div class=\"h_link\"><a href=\"'+str(r[3])+'\">'+str(r[3])+'</a></div>'\n",
    "        rhtm +='<div class=\"h_city\">City: '+str(r[2])+'</div>'\n",
    "        rhtm +='<div class=\"h_disc\">'+r[1].replace('\\n', '')[:300]+'... </div>'\n",
    "        rhtm +='<div class=\"h_score\"> Similarity Score: '+str(r[4])+'... </div>'\n",
    "        rhtm +='</div>'\n",
    "        return (rhtm)\n",
    "    \n",
    "    def printResults(self, query, k=10):\n",
    "        docs = self.getDocsByQuery(query, k)\n",
    "        styles = open(\"style/style.css\", \"r\").read()\n",
    "        hhtm = '<style>%s</style>' % styles     \n",
    "        hhtm +='<div class=\"Hoohle\" ><div class=\"h_results\">'\n",
    "        if docs != None:\n",
    "            for doc in docs:\n",
    "                hhtm += self.printResult(self.getDetails(doc[0], query))\n",
    "        else:\n",
    "            hhtm +='Sorry! No results found for your request!'\n",
    "        hhtm += '</div></div>'\n",
    "        display(HTML(hhtm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HoohleNOSTRO Class (Step 4)\n",
    "\n",
    "This class represent the third search engine, and it inherits the first search engine class __(HoohleSimple)__ \n",
    "The object of this class takes the data (airbnbData) object which will be searched and boolean value if it needs to create the index or read it from file.\n",
    "\n",
    "*For the explanation of our approach in defining the scores, please refer to the main markdown document.*\n",
    "\n",
    "This class builds a new index __indexNOSTRO__ which contains each document with the a list of the scores (price, rooms, distance).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class HoohleNOSTRO\n",
    "class HoohleNOSTRO(HoohleSimple):\n",
    "    def __init__(self, airbnb, buildIndex=False):\n",
    "        HoohleSimple.__init__(self, airbnb)\n",
    "        self.data = airbnb\n",
    "        self.data.data['price'] = [float(str(i).replace('$','')) for i in self.data.data['average_rate_per_night']]\n",
    "        self.maxPrice = self.data.data['price'].max()\n",
    "        self.minPrice = self.data.data['price'].min()\n",
    "        \n",
    "        self.data.data['rooms'] = self.data.data['bedrooms_count'].replace({'Studio':'1'}).astype(float).fillna(0.0)\n",
    "        self.maxRooms = self.data.data['rooms'].max()\n",
    "        self.minRooms = self.data.data['rooms'].min()\n",
    "        \n",
    "        if buildIndex: self.buildIndex()\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SaveIndex()\n",
    "#### readIndex()\n",
    "#### saveGeoDict()\n",
    "\n",
    "Functions to save/read the index and GeoDict to/from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # two functions to save and read index to/from a file\n",
    "    def saveIndex(self, file):\n",
    "        deepdish.io.save(file, self.indexNostro)\n",
    "    def readIndex(self, file):\n",
    "        self.indexNostro = deepdish.io.load(file)  \n",
    "    def saveGeoDict(self):\n",
    "        self.geodict = deepdish.io.load(file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scoringprice()\n",
    "This function gets the max and min price of all entries and assigns a score to the a given price based on its position along the range of prices (from min to max).\n",
    "__There's no absolute meaning for value of this score, it is only used to get the similarity of two prices__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Price scoring algorithm: is taking the max price as the value with score 1, and the min value with score = 0\n",
    "    # NOTE: it doesnt reflect any real absolute value, \n",
    "    # but when it's combined with the price score of the query it allow us to get the similarity between the result and the query\n",
    "    def scoringPrice(self, pr):\n",
    "        if pr == 0 or pr == None: return (0)\n",
    "        if pr >= self.maxPrice: return (1)\n",
    "        return((pr - self.minPrice)/(self.maxPrice - self.minPrice))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scoringRoom()\n",
    "Same approach as in the previous function, it runs on number of rooms data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # same explaination\n",
    "    def scoringRoom(self, r):\n",
    "        if r == 0 or r == None: return (0)\n",
    "        if r >= self.maxRooms : return (1)\n",
    "        return ((r - self.minRooms)/(self.maxRooms - self.minRooms))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### buildGeoInfo()\n",
    "This function gets coordinates of the city centers of our cities and stores them in a dictionary __geodict__.\n",
    "It uses an open source API __geopy__ provided by __openstreetmap__.\n",
    "\n",
    "It first gets a list of unique city values in data and call the API to get the coordinates of each city center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def buildGeoInfo(self):\n",
    "        from geopy.geocoders import Nominatim\n",
    "        self.geolocator = Nominatim(user_agent=\"ADM201-HW3\")\n",
    "        from geopy.exc import GeocoderTimedOut\n",
    "    \n",
    "        # getting the list of unique cities\n",
    "        self.uniqueCity = self.data.data['city'].unique()\n",
    "        \n",
    "        self.geodict = {}\n",
    "        for city in self.uniqueCity:\n",
    "            try:\n",
    "                location = self.geolocator.geocode(city+\" TX\", timeout=10)\n",
    "                if location != None:\n",
    "                    self.geodict[city] = [location.latitude, location.longitude]\n",
    "                else:\n",
    "                    self.geodict[city] = [None, None]\n",
    "            except GeocoderTimedOut as e:\n",
    "                print(\"Error: geocode failed \")\n",
    "        \n",
    "        #some values couldnt be retrieved by function, so we got them manually: \n",
    "        self.geodict['North Padre Island Corpus Christi'] = [27.800583,-97.396378]\n",
    "        self.geodict['Boliver peninsula'] = [29.562353,-94.394371]\n",
    "        self.geodict['Bolivar Pennisula'] = [29.562353,-94.394371]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### insetGeoInfo()\n",
    "This function uses the coordinate information of the city centers to calculate the distance in __km__ for each house to the city center of The city it exists in, by calling __geopy.distance__ module and stores the values in a new column in the data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def insetGeoInfo(self):\n",
    "        self.data.data['ccLat']= [self.geodict[x][0] for x in self.data.data.city]\n",
    "        self.data.data['ccLong']= [self.geodict[x][1] for x in self.data.data.city]\n",
    "        \n",
    "        self.data.data['distanceToCC'] = self.data.data.apply(lambda row: geopy.distance.vincenty((row.latitude,row.longitude), (row.ccLat,row.ccLong)).km, axis=1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scoringLoc()\n",
    "This function calculates the location score for each entry (house), based on its distance to the city center.\n",
    "So the closer the flat is to city center the higher location score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # location scores\n",
    "    def scoringLoc(self, city, dista):\n",
    "        maxDist = self.data.data[self.data.data['city']==city]['distanceToCC'].max()\n",
    "        if dista >= maxDist: return (0.0)\n",
    "        return (1 - (dista/maxDist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### buildIndex()\n",
    "\n",
    "This function builds the indexNostro, using the previous three scoring functions (for price, rooms and location).\n",
    "The __indexNostro__ is a disctionay which contains an entry for each house with a vector of its three scores (price, room, location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # creating the new index\n",
    "        \n",
    "    def buildIndex(self):\n",
    "        self.indexNostro = {}\n",
    "        for index,row in self.data.data.iterrows():\n",
    "            self.indexNostro[index+1] = [self.scoringPrice(row.price), self.scoringRoom(int(row.rooms)), self.scoringLoc(row.city, row.distanceToCC)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rank()\n",
    "This function sorts the list of documents based on their rank of cosine similarity to the information provided by the user.\n",
    "It uses __heap__ data structure as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # function to order results by their ranking\n",
    "    def rank(self, vv, res):\n",
    "        resHeap = []\n",
    "        for doc in res:\n",
    "            resHeap.append([doc, getCosineSimi(self.indexNostro[doc], vv)])\n",
    "        return(hq.nlargest(len(resHeap), resHeap, key=lambda x:x[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gimmeTheScore()\n",
    "\n",
    "This function returns a vector of scores for a given lsit of values provided by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # function to calculate the score of user input\n",
    "    def gimmeTheScore(self, vv):\n",
    "        return ([self.scoringPrice(vv[0]), self.scoringRoom(vv[1]), self.scoringLoc(vv[2], vv[3])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getDetails()\n",
    "\n",
    "Its the same as the __Property__ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # funciton retuns the details of a file given file id\n",
    "\n",
    "    def getDetails(self, fid):\n",
    "        detlist =[]\n",
    "        p = Property(fid, True)\n",
    "        \n",
    "        return([p.title, p.description, p.city, p.url])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### showUI()\n",
    "This function shows the input text boxes to the user and returns the values provided and then passes them to the printing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    def showUI(self):\n",
    "        qq = input(\"What are you looking for?\")\n",
    "        qprice = input(\"price?\")\n",
    "        qrooms = input(\"number of rooms?\")\n",
    "        qcity = input(\"In which city?\")\n",
    "        qdist = input(\"distance to city center?\")\n",
    "        self.printResults(self, qq, [float(qprice), float(qrooms), qcity, float(qdist)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### printResult()\n",
    "#### printResults()\n",
    "\n",
    "Same functions in __HoohleSimple__ class.\n",
    "But it returns the rank attached to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def printResult(self, r,rank):\n",
    "        rhtm ='<div class=\"h_result\"><div class=\"h_rank\">'+str(rank)+'</div>'\n",
    "        rhtm +='<div class=\"h_title\"><a href=\"'+r[3]+'\">'+r[0]+'</a></div>'\n",
    "        rhtm +='<div class=\"h_link\"><a href=\"'+r[3]+'\">'+r[3]+'</a></div>'\n",
    "        rhtm +='<div class=\"h_city\">City: '+str(r[2])+'</div>'\n",
    "        rhtm +='<div class=\"h_disc\">'+r[1].replace('\\n', '')[:300]+'... </div>'\n",
    "        rhtm +='</div>'\n",
    "        return (rhtm)\n",
    "\n",
    "    def printResults(self, query, v):\n",
    "        res = self.getDocsByQuery(query)\n",
    "        vv = self.gimmeTheScore(v)\n",
    "        docs = self.rank(vv, res)\n",
    "        styles = open(\"style/style.css\", \"r\").read()\n",
    "        hhtm = '<style>%s</style>' % styles     \n",
    "        hhtm +='<div class=\"Hoohle\" ><div class=\"h_results\">'\n",
    "        if docs != None:\n",
    "            rank=1\n",
    "            for doc in docs:\n",
    "#                p = Property(doc[0], True)\n",
    "                hhtm +=self.printResult(self.getDetails(doc[0]),rank)\n",
    "                rank+=1\n",
    "        else:\n",
    "            hhtm +='Sorry! No results found for your request!'\n",
    "        hhtm += '</div></div>'\n",
    "        display(HTML(hhtm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
