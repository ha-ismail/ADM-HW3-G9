{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading tsvs and getting cleaned stemmed dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwords = []\\ndef getWordsFromTSV(fid):\\n#    with open('data/doc_'+fid+'.tsv') as fd:\\n    with open('doc_1.tsv') as fd:\\n        data = fd.read()\\n        return(data)\\n\\nprint(getWordsFromTSV(1))\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "words = []\n",
    "def getWordsFromTSV(fid):\n",
    "#    with open('data/doc_'+fid+'.tsv') as fd:\n",
    "    with open('doc_1.tsv') as fd:\n",
    "        data = fd.read()\n",
    "        return(data)\n",
    "\n",
    "print(getWordsFromTSV(1))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the given dataset into pandas for processing using only the needed columns\n",
    "data = pd.read_csv('Airbnb_Texas_Rentals.csv', nrows=10, usecols= ['average_rate_per_night', 'bedrooms_count','city', 'date_of_listing', 'description', 'latitude','longitude','title', 'url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a file for every row in data(the given data of airbnb)\n",
    "fileCount = 0\n",
    "for index, r in data.iterrows():\n",
    "    data_temp = data.loc[index:index]\n",
    "    fileCount +=1\n",
    "    data_temp.to_csv('data/doc_'+str(index+1)+'.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE THIS PART \n",
    "# function to extract unique words from tsv file and stor them in (words)\n",
    "# it also remove \\n , puctuations and stopwords\n",
    "# stem words\n",
    "# TODO: create a separate function for the cleaning and stemming stuff, and call it inside the below function\n",
    "# TODO: we need to stem before checking if the words is already there\n",
    "# TODO: create funtion to extract only needed data from a file, then we can use in creating the index\n",
    "\n",
    "def NO_getWordsTSV(fid):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    data = [i.strip('\\n').split('\\t') for i in open('data/doc_'+str(fid)+'.tsv')]\n",
    "    if ((data[0][2] not in words) and (data[0][2] not in stop_words)):words.append(data[0][2].strip())\n",
    "    for w in (tokenizer.tokenize(data[0][4].replace('\\\\n', ' '))):\n",
    "        \n",
    "        if (w not in words and w not in stop_words): words.append(stemmer.stem(w))\n",
    "#words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'googletrans'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b9349d231d9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnowball\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogletrans\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'googletrans'"
     ]
    }
   ],
   "source": [
    "# Organizing the functions\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from googletrans import Translator\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "translator = Translator()\n",
    "\n",
    "# list of unique words (terms) to be used to build the vocabulary\n",
    "words = []\n",
    "\n",
    "def cleanData(rawData):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # TODO translatoin\n",
    "    #lang = detect(t4[0])\n",
    "    #print(lang)\n",
    "    #rawData = translator.translate(rawData, dest='en')\n",
    "    \n",
    "    # get words lowercased\n",
    "    t0 = rawData.lower()\n",
    "    # remove puctuations\n",
    "    t1 = tokenizer.tokenize(t0)\n",
    "    #t2Data = stemmer.stem(i for i in t1Data)\n",
    "    #print(len(t1))\n",
    "    # reomve stop words\n",
    "    t2 =[]\n",
    "    t2 = [t1[i] for i in range(0,len(t1)) if t1[i] not in stop_words]\n",
    "    \n",
    "    # stemm words\n",
    "    t3 = [stemmer.stem(t2[i]) for i in range(0, len(t2))]\n",
    "    \n",
    "    # remove nummbers and strings starting with numbers\n",
    "    t4 = [t3[i] for i in range(0, len(t3)) if t3[i][0].isdigit()==False]\n",
    "    \n",
    "\n",
    "    #print(t4)\n",
    "    return(t4)\n",
    "\n",
    "def getWordsTSV(fid):\n",
    "    fileWords=[]\n",
    "    for i in open('data/doc_'+str(fid)+'.tsv'):\n",
    "        #words.append(cleanData(i[0][4]))\n",
    "        data = [i.strip('\\n').split('\\t') for i in open('data/doc_'+str(fid)+'.tsv')]\n",
    "        for w in (cleanData(data[0][4].replace('\\\\n', ' '))):\n",
    "            if w not in fileWords: fileWords.append(w)\n",
    "    return(fileWords)\n",
    "\n",
    "#defining new function to get the words of the files for indexing\n",
    "def getWordsTSV_idx(fid):\n",
    "    words_idx= []\n",
    "    for i in open('data/doc_'+str(fid)+'.tsv',encoding=\"utf8\"):\n",
    "        #words.append(cleanData(i[0][4]))\n",
    "        data = [i.strip('\\n').split('\\t') for i in open('data/doc_'+str(fid)+'.tsv', encoding=\"utf8\")]\n",
    "        for w in (cleanData(data[0][4].replace('\\\\n', ' '))):\n",
    "            words_idx.append(w)\n",
    "    return(words_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*getWordsTSV(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-5810a8a963b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfileCount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetWordsTSV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1,fileCount):\n",
    "    t = getWordsTSV(i)\n",
    "    for w in t:\n",
    "        if w not in words:\n",
    "            words.append(w)\n",
    "print(*words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PS\n",
    "dunno why the stemmer is removing the last 'e' from any word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*words)\n",
    "#words=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reset the words \n",
    "#words =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the vocabulary file from the words \n",
    "wordsCount=0\n",
    "with open('vocabulary.csv','wb') as vfile:\n",
    "    for i in range(0,len(words)):\n",
    "        vfile.write(str(wordsCount).encode())\n",
    "        vfile.write(str('\\t').encode())\n",
    "        vfile.write(str(words[i]).encode())\n",
    "        vfile.write('\\n'.encode())\n",
    "        wordsCount+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['德州农机大学', 'texa', 'amp', 'univers', '北门附近2b1b其中的一间卧室提供日租', '位置便利', '步行进入校区只需要5分钟', '室友为中国男生', '好相处', '日租时间段为2015年12月9日', '适合刚来学校没有车或者不打算买车的同学和老师', '男性']\n"
     ]
    }
   ],
   "source": [
    "print(cleanData(\"德州农机大学(Texas A&amp;M University)北门附近2b1b其中的一间卧室提供日租。位置便利,步行进入校区只需要5分钟;室友为中国男生,好相处;日租时间段为2015年12月9日~30日。适合刚来学校没有车或者不打算买车的同学和老师(男性)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import translate\n",
    "translate_client = translate.Client()\n",
    "text = u'Hello, world!'\n",
    "target = 'ru'\n",
    "# Translates some text into Russian\n",
    "translation = translate_client.translate(\n",
    "    text,\n",
    "    target_language=target)\n",
    "\n",
    "print(u'Text: {}'.format(text))\n",
    "print(u'Translation: {}'.format(translation['translatedText']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, fileCount):\n",
    "    for j in ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating index-file by loading vocabulary file and then comparing files with all vocabularys\n",
    "with open('vocabulary.csv', newline='') as file:\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    voc = list(map(tuple, reader))\n",
    "#print(voc)\n",
    "\n",
    "index = {}\n",
    "for i in range(len(voc)):\n",
    "    tempL=[]\n",
    "\n",
    "    for j in range(1,fileCount+1):\n",
    "        temp = getWordsTSV_idx(j)\n",
    "        #print(temp)\n",
    "        if voc[i][1] in temp:\n",
    "            tempL.append(j)\n",
    "            \n",
    "        if len(tempL)!=0: index[i]= tempL\n",
    "            \n",
    "print(index)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
